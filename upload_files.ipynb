{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Code for uploading files to S3 Bucket"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "# ! pip install pandarallel\n",
    "import re, string\n",
    "from bs4 import BeautifulSoup\n",
    "import spacy\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "# pip install lxml\n",
    "import boto3\n",
    "from tqdm import tqdm\n",
    "# !python -m spacy download en_core_web_sm\n",
    "nlp = spacy.load(\"en_core_web_sm\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "s3 = boto3.resource('s3')\n",
    "BUCKET_NAME = 'ethan-kozlowski-project'\n",
    "\n",
    "s3.create_bucket(Bucket=BUCKET_NAME)\n",
    "bucket = s3.Bucket(BUCKET_NAME)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "def upload_to_s3(file_name:str, bucket:str, object_name:str=None, s3=boto3.client('s3')):\n",
    "    \"\"\"\n",
    "    Upload a file to an S3 bucket return True if successful, else False\n",
    "    \"\"\"\n",
    "    # if no object name is given, just make it the same as the file name\n",
    "    if object_name is None:\n",
    "        object_name = file_name\n",
    "\n",
    "    try:\n",
    "        s3.upload_file(file_name, bucket, object_name)\n",
    "    except Exception as e:\n",
    "        print(e)\n",
    "        return False\n",
    "    return True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "old_data = r\"D:\\hw\\adopt_proj\\DATA_DO_NOT_REMOVE\\all_posts_pre_cleaned.csv\"\n",
    "newer_posts = r\"D:\\hw\\adopt_proj\\DATA_DO_NOT_REMOVE\\CRAWLEDnewest_posts_5-9.csv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "old_data_df = pd.read_csv(old_data)\n",
    "newer_posts_df = pd.read_csv(newer_posts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "full_df = pd.concat([old_data_df, newer_posts_df]\n",
    "                    ).drop_duplicates(\n",
    "                        subset=\"id\"\n",
    "                    ).reset_index(\n",
    "                        drop=True\n",
    "                    ).drop(\n",
    "                        columns=['url', 'upvote_ratio']\n",
    "                    )\n",
    "full_df[\"time\"] = pd.to_datetime(full_df[\"time\"], unit='s')\n",
    "full_df[\"title\"] = full_df.title.fillna(\"\")\n",
    "full_df[\"text\"] = full_df.text.fillna(\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to check if a string ends with punctuation and return the appropriate string\n",
    "def format_full_text(title, text):\n",
    "    if not title:\n",
    "        full_text = text\n",
    "    elif title.endswith(('.', '!', '?')):\n",
    "        full_text = f\"{title} {text}\"\n",
    "    else:\n",
    "        full_text = f\"{title}. {text}\"\n",
    "   \n",
    "    return full_text.strip().lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "full_df[\"full_text\"] = full_df.apply(lambda row: format_full_text(row.title, row.text), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "replace_dict = {\n",
    "    '’': \"'\",\n",
    "    '‘': \"'\",\n",
    "    '“': '\"',\n",
    "    '”': '\"',\n",
    "    \"•\": \"\",\n",
    "    \"…\": \" \",\n",
    "    \"&\": \"and\",\n",
    "    \"è\": \"e\",\n",
    "    \"é\": \"e\",\n",
    "    \"ê\": \"e\",\n",
    "    \"à\": \"a\",\n",
    "    \"â\": \"a\",\n",
    "    \"ô\": \"o\",\n",
    "    \"û\": \"u\",\n",
    "    \"ç\": \"c\",\n",
    "    \"î\": \"i\",\n",
    "    \"ï\": \"i\",\n",
    "    \"ù\": \"u\",\n",
    "    \"ü\": \"u\",\n",
    "    \"°\": \"degree\"\n",
    "}\n",
    "\n",
    "# replace these problematic characters especially for contractions used later\n",
    "full_df[\"full_text\"] = full_df.full_text.replace(replace_dict, regex=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "# change the ID tags by removing the 't\\d_' prefix\n",
    "full_df.parent_id = full_df.apply(lambda row: re.sub(r't\\d_', \"\", row.parent_id) if pd.notna(row.parent_id) else row.id, axis=1)\n",
    "full_df.link_id = full_df.apply(lambda row: re.sub(r't\\d_', \"\", row.link_id) if pd.notna(row.link_id) else row.id, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['exclaim_bot', 'molliebot', 'Booty_Warrior_bot', 'could-of-bot',\n",
       "       'sneakpeekbot', 'receptionist_robot', 'converter-bot',\n",
       "       'of_patrol_bot', 'tiny_smile_bot', 'hotlinehelpbot', 'haikusbot',\n",
       "       'serendipitybot', 'haiku-testbot', 'useles-converter-bot',\n",
       "       'nicolesarobot', 'ectbot', 'overthinkingrobot', 'battbot',\n",
       "       'totes_meta_bot', 'autowikibot', 'video_descriptionbot',\n",
       "       '007Fembot', 'icarebot', 'linebreaker-bot', 'conversionbot',\n",
       "       'wikipedia_answer_bot', 'cheer_up_bot', 'octopus_tigerbot',\n",
       "       'JordanTheBrobot', 'EncouragementRobot', 'Lesbianadoptibot',\n",
       "       'mortalitybot', 'image_linker_bot', 'HIPPAbot', 'IrishReplybot',\n",
       "       'imdad_bot', 'youtubefactsbot', 'alternate-source-bot',\n",
       "       'outline_link_bot', 'hipaa-bot', 'I_am_a_haiku_bot',\n",
       "       'yourewelcome_bot', 'yourewelcome_botbot', 'these_days_bot',\n",
       "       'nice___bot', 'resavr_bot', 'metric_robot', 'Chuck_Norris_Jokebot'],\n",
       "      dtype=object)"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "full_df[full_df.author.str.contains(r\"bot$\", na=False)].author.unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "# potential bots \n",
    "\n",
    "problem_users = [\"I_am_a_haiku_bot\", \n",
    "                 \"happy-cake-day-bot-\", \n",
    "                 \"Chuck_Norris_Jokebot\",\n",
    "                 \"yourewelcome_bot\",\n",
    "                 \"yourewelcome_botbot\",\n",
    "                 \"nice___bot\",\n",
    "                 \"Booty_Warrior_bot\",\n",
    "                 \"useles-converter-bot\",\n",
    "                 \"IrishReplybot\",\n",
    "                 \"hipaa-bot\",\n",
    "                 \"metric_robot\",\n",
    "                 \"autowikibot\",\n",
    "                 \"resavr_bot\",\n",
    "                 \"ectbot\",\n",
    "                 \"exclaim_bot\",\n",
    "                 \"EncouragementRobot\",\n",
    "                 \"alternate-source-bot\",\n",
    "                 \"could-of-bot\",\n",
    "                 \"sneakpeekbot\",\n",
    "                 \"youtubefactsbot\",\n",
    "                 \"video_descriptionbot\",\n",
    "                 \"conversionbot\",\n",
    "                 \"serendipitybot\",\n",
    "                 \"Squirrelslayer777\", # this user is a troll\n",
    "                 \"wikipedia_answer_bot\",\n",
    "                 \"tiny_smile_bot\",\n",
    "                 \"hotlinehelpbot\",\n",
    "                 \"outline_link_bot\",\n",
    "                 \"totes_meta_bot\",\n",
    "                 \"icarebot\",\n",
    "                 \"linebreaker-bot\",\n",
    "                 \"JordanTheBrobot\",\n",
    "                 \"mortalitybot\",\n",
    "                 \"image_linker_bot\",\n",
    "                 \"imdad_bot\"]\n",
    "\n",
    "bots = pd.read_csv(r\"D:\\hw\\adopt_proj\\DATA_DO_NOT_REMOVE\\bots.csv\", header=None)\n",
    "known_bots = set(bots[0]) | set(problem_users)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.Series(list(known_bots)).to_csv('bots.csv', index=False, header=False, \n",
    "                                   encoding='utf-8', escapechar='\\\\', quotechar='\"', quoting=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove bots\n",
    "full_df = full_df[~full_df.author.isin(known_bots)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_equivalents = {\n",
    "    \"adoptive mom\": \"amom\",\n",
    "    \"amother\": \"amom\",\n",
    "    \"adoptive sister\": \"asis\",\n",
    "    \"adoptive sis\": \"asis\",\n",
    "    \"adoptive brother\": \"abro\",\n",
    "    \"adoptive bro\" : \"abro\",\n",
    "    \"adoptive mother\": \"amom\",\n",
    "    \"adoptive dad\": \"adad\",\n",
    "    \"afather\": \"adad\",\n",
    "    \"adoptive father\": \"adad\",\n",
    "    \"adoptive parent\": \"ap\",\n",
    "    \"adoptive parents\": \"aps\",\n",
    "    \"bio mom\": \"bmom\",\n",
    "    \"bmom\": \"bmom\",\n",
    "    \"bio mother\": \"bmom\",\n",
    "    \"biomother\" :\"bmom\",\n",
    "    \"biomom\": \"bmom\",\n",
    "    \"birthmom\": \"bmom\",\n",
    "    \"birth mom\": \"bmom\",\n",
    "    \"birth mother\": \"bmom\",\n",
    "    \"biological mom\": \"bmom\",\n",
    "    \"biological mother\": \"bmom\",\n",
    "    \"birth mom\": \"bmom\",\n",
    "    \"biological dad\": \"bdad\",\n",
    "    \"biological father\": \"bdad\",\n",
    "    \"bio dad\": \"bdad\",\n",
    "    \"bdad\": \"bdad\",\n",
    "    \"biodad\": \"bdad\",\n",
    "    \"biofather\": \"bdad\",\n",
    "    \"bio father\": \"bdad\",\n",
    "    \"birth dad\": \"bdad\",\n",
    "    \"birth father\": \"bdad\",\n",
    "    \"birth parent\": \"bp\",\n",
    "    \"biological parent\": \"bp\",\n",
    "    \"bio parent\": \"bp\",\n",
    "    \"bio parents\": \"bps\",\n",
    "    \"birth parents\": \"bps\",\n",
    "    \"biological parents\": \"bps\",\n",
    "    \"birth sister\": \"bsis\",\n",
    "    \"bio sis\": \"bsis\",\n",
    "    \"bio sister\": \"bsis\",\n",
    "    \"birth brother\": \"bbro\",\n",
    "    \"bio bro\": \"bbro\",\n",
    "    \"bio brother\": \"bbro\",\n",
    "    \"birth sibling\": \"bsibling\",\n",
    "    \"bio sibling\": \"bsibling\",\n",
    "    \"bio sib\": \"bsibling\",\n",
    "    \"bio family\": \"birthfamily\",\n",
    "    \"birth family\": \"birthfamily\",\n",
    "    \"biological family\": \"birthfamily\",\n",
    "    \"adoptive family\": \"adoptive_family\",\n",
    "    \"first mom\": \"first_mom\",\n",
    "    \"first mother\": \"first_mom\",\n",
    "    \"first dad\": \"first_dad\",\n",
    "    \"first father\": \"first_dad\",\n",
    "    \"first family\": \"first_family\",\n",
    "    \"first fam\": \"first_family\",\n",
    "    \"step mom\": \"stepmom\",\n",
    "    \"stepmother\": \"stepmom\",\n",
    "    \"stepfather\": \"stepdad\",\n",
    "    \"step mother\": \"stepmom\",\n",
    "    \"step dad\": \"stepdad\",\n",
    "    \"step father\": \"stepdad\",\n",
    "    \"step parent\": \"stepparent\",\n",
    "    \"step parents\": \"stepparents\",\n",
    "    \"step sister\": \"stepsister\",\n",
    "    \"step brother\": \"stepbrother\",\n",
    "    \"step sibling\": \"stepsibling\",\n",
    "    \"stepfamily\": \"step_family\",\n",
    "    \"stepfamilies\": \"step_families\",\n",
    "    \"step families\": \"step_families\",\n",
    "    \"step family\": \"step_family\",\n",
    "    \"step sis\": \"stepsister\",\n",
    "    \"step bro\": \"stepbrother\",\n",
    "    \"step son\": \"stepson\",\n",
    "    \"step daughter\": \"stepdaughter\",\n",
    "    \"adoptive son\" : \"adoptive_son\",\n",
    "    \"adoptive daughter\": \"adoptive_daughter\",\n",
    "    \"adoptive child\": \"adoptive_child\",\n",
    "    \"foster mom\": \"fostermom\",\n",
    "    \"foster parent\": \"fosterparent\",\n",
    "    \"foster parents\": \"fosterparents\",\n",
    "    \"fostermother\": \"fostermom\",\n",
    "    \"foster mother\": \"fostermom\",\n",
    "    \"foster family\": \"foster_family\",\n",
    "    \"foster families\" : \"foster_families\",\n",
    "    \"foster dad\": \"fosterdad\",\n",
    "    \"foster father\": \"fosterdad\",\n",
    "    \"fosterfather\": \"fosterdad\",\n",
    "    \"transracial adoptee\": \"tra\",\n",
    "    \"half sister\": \"halfsister\",\n",
    "    \"half brother\": \"halfbrother\",\n",
    "    \"half sibling\": \"halfsibling\",\n",
    "    \"half siblings\": \"halfsiblings\",\n",
    "    \"half sis\": \"halfsister\",\n",
    "    \"half bro\": \"halfbrother\",\n",
    "    \"birth certificate\" : \"birth_certificate\",\n",
    "    \"et cetera\": \"etc\",\n",
    "    \"home town\": \"hometown\",\n",
    "    \"home country\": \"homecountry\",\n",
    "    \"birth place\": \"birthplace\",\n",
    "    \"birth country\": \"birthcountry\",\n",
    "    \"prospective adoptive parent\" : \"pap\",\n",
    "    \"father-in-law\": \"father_in_law\",\n",
    "    \"mother-in-law\": \"mother_in_law\",\n",
    "    \"brother-in-law\": \"brother_in_law\",\n",
    "    \"sister-in-law\": \"sister_in_law\",\n",
    "    \"mum-in-law\": \"mother_in_law\",\n",
    "    \"father in law\": \"father_in_law\",\n",
    "    \"brother in law\": \"brother_in_law\",\n",
    "    \"mother in law\": \"mother_in_law\",\n",
    "    \"sister in law\": \"sister_in_law\",\n",
    "    \"mum in law\": \"mother_in_law\",\n",
    "    \"birthmum\": \"bmom\",\n",
    "    \"birth mum\": \"bmom\",\n",
    "    \"bio mum\": \"bmom\",\n",
    "    \"biological mum\": \"bmom\",\n",
    "    \"biomum\": \"bmom\",\n",
    "    \"bmomma\": \"bmom\",\n",
    "    \"son in law\": \"son_in_law\",\n",
    "    \"daughter in law\": \"daughter_in_law\",\n",
    "    \"ad ptee\": \"adoptee\",\n",
    "    \"intercountry adoptee\": \"ica\",\n",
    "    \"intercountry adoption\": \"ica\",\n",
    "    \"united states\": \"usa\",\n",
    "    \"united kingdom\": \"uk\",\n",
    "    \"new york\": \"ny\",\n",
    "    \"los angeles\": \"la\",\n",
    "    \"former foster youth\": \"ffy\",\n",
    "    \"puerto ric\": \"puerto_ric\",\n",
    "    \"child protective services\": \"cps\",\n",
    "    \"foster care\": \"fostercare\",\n",
    "    \"south korea\": \"sk\",\n",
    "    \"hong kong\": \"hk\",\n",
    "    \"fiancee\": \"fiance\",\n",
    "    \"fiancees\": \"fiances\",\n",
    "    \"girl friend\": \"girlfriend\",\n",
    "    \"boy friend\": \"boyfriend\",\n",
    "    \"tl dr\": \"tldr\",\n",
    "    \"tl:dr\": \"tldr\",\n",
    "    \"family tree dna\": \"ftdna\", # an ancestry dna website\n",
    "    \"bipolar depression\": \"bpd\",\n",
    "    \"bi polar depression\": \"bpd\",\n",
    "    \"pod cast\": \"podcast\",\n",
    "    \"post traumatic stress disorder\": \"ptsd\",\n",
    "    \"up vote\": \"upvote\",\n",
    "    \"down vote\": \"downvote\",\n",
    "}\n",
    "\n",
    "word_equiv_pattern = re.compile('|'.join(re.escape(key) for key in word_equivalents.keys()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "def decontracted(phrase):\n",
    "    \"\"\"\n",
    "    Expand the contracted phrase into normal words\n",
    "    \"\"\"\n",
    "    # specific\n",
    "    phrase = re.sub(r\"\\bwon't\\b\", \"will not\", phrase)\n",
    "    phrase = re.sub(r\"\\bcan\\'t\\b\", \"can not\", phrase)\n",
    "\n",
    "    # general\n",
    "    phrase = re.sub(r\"n\\'t\\b\", \" not\", phrase)\n",
    "    phrase = re.sub(r\"\\'re\\b\", \" are\", phrase)\n",
    "    phrase = re.sub(r\"\\'s\\b\", \" is\", phrase) \n",
    "    phrase = re.sub(r\"\\'d\\b\", \" would\", phrase)\n",
    "    phrase = re.sub(r\"\\'ll\\b\", \" will\", phrase)\n",
    "    phrase = re.sub(r\"\\'t\\b\", \" not\", phrase)\n",
    "    phrase = re.sub(r\"\\'ve\\b\", \" have\", phrase)\n",
    "    phrase = re.sub(r\"\\'m\\b\", \" am\", phrase)\n",
    "    \n",
    "    return phrase\n",
    "\n",
    "\n",
    "def remove_excess_punctuation(text):\n",
    "    # Remove all but the last punctuation in a mixed sequence, and reduce any subsequent identical marks\n",
    "    text = re.sub(r'([.!?])\\s*([.!?]\\s*)+', r'\\1', text)\n",
    "\n",
    "    # Correct spacing around punctuation\n",
    "    text = re.sub(r'([.!?])([^\\s])', r'\\1 \\2', text)\n",
    "\n",
    "    # Trim spaces around end punctuation and remove redundant end punctuation\n",
    "    text = re.sub(r'\\s*([.!?])\\s*$', r'\\1', text)\n",
    "    \n",
    "    # Remove isolated punctuation marks within the text\n",
    "    text = re.sub(r'\\s+([!?])\\s+', ' ', text)\n",
    "\n",
    "    return text.strip()\n",
    "\n",
    "\n",
    "def clean_text(text):\n",
    "    \"\"\"\n",
    "    Clean the review texts\n",
    "    \"\"\"\n",
    "   \n",
    "    # expand the contracted words\n",
    "    post_text = decontracted(text)\n",
    "   \n",
    "    # remove html tags\n",
    "    post_text = BeautifulSoup(post_text, 'lxml').get_text().strip() # re.sub(r'<.*?>', '', text)\n",
    "       \n",
    "    # remove url \n",
    "    post_text = re.sub(r'https?://\\S+|www\\.\\S+', '', post_text)\n",
    "    # remove emails\n",
    "    post_text = re.sub(r\"(^[a-zA-Z0-9_.+-]+@[a-zA-Z0-9-]+\\.[a-zA-Z0-9-.]+$)\", '', post_text)\n",
    "    \n",
    "\n",
    "\n",
    "    # remove special characters\n",
    "    post_text = re.sub(r'[^a-z\\!\\?\\.]+', ' ', post_text)\n",
    "    # post_text = re.sub(r'\\\"|\\,|\\/|\\#|\\$|\\%|\\&|\\'|\\(|\\)|\\:|\\*|\\+|\\<|\\=|\\>|\\@|\\[|\\]|\\^|\\_|\\{|\\||\\}|\\~|\\`|…|\\-|–|—|[0-9]', \" \" , post_text)\n",
    "    # remove elipsses and artifacts of empty sentences\n",
    "    post_text = remove_excess_punctuation(post_text)\n",
    "    # replace subreddit mentions\n",
    "    post_text = re.sub(word_equiv_pattern, lambda x: word_equivalents[x.group(0)], post_text)\n",
    "\n",
    "    reddit_pattern = r\"\\b(\\/r\\/|r\\/|r )([a-zA-Z0-9][a-zA-Z0-9_]{0,20})\\b\"\n",
    "    post_text = re.sub(reddit_pattern, lambda m: 'r_' + m.group(2), post_text)\n",
    "    # replace user name mentions\n",
    "    reddit_user_name_pattern = r\"\\b(\\/u\\/|u\\/|u )([a-zA-Z0-9_-]{2,20})\\b\"\n",
    "    post_text = re.sub(reddit_user_name_pattern, lambda m: 'u_' + m.group(2), post_text) \n",
    "    # remove extra white space\n",
    "    post_text = re.sub(r\"[\\s ]+\", \" \", post_text)\n",
    "    \n",
    "    return post_text.strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/299893 [00:00<?, ?it/s]C:\\Users\\Ethan\\AppData\\Local\\Temp\\ipykernel_22624\\4041638121.py:47: MarkupResemblesLocatorWarning: The input looks more like a filename than markup. You may want to open this file and pass the filehandle into Beautiful Soup.\n",
      "  post_text = BeautifulSoup(post_text, 'lxml').get_text().strip() # re.sub(r'<.*?>', '', text)\n",
      "  2%|▏         | 4522/299893 [00:01<01:54, 2584.04it/s]C:\\Users\\Ethan\\AppData\\Local\\Temp\\ipykernel_22624\\4041638121.py:47: MarkupResemblesLocatorWarning: The input looks more like a URL than markup. You may want to use an HTTP client like requests to get the document behind the URL, and feed that document to Beautiful Soup.\n",
      "  post_text = BeautifulSoup(post_text, 'lxml').get_text().strip() # re.sub(r'<.*?>', '', text)\n",
      "100%|██████████| 299893/299893 [01:53<00:00, 2639.39it/s]\n"
     ]
    }
   ],
   "source": [
    "tqdm.pandas()\n",
    "full_df[\"full_text\"] = full_df.full_text.progress_apply(clean_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": [
    "full_df = full_df[full_df.author.notna()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "sorted_flairs = pd.read_csv(r\"D:\\hw\\adopt_proj\\DATA_DO_NOT_REMOVE\\all_sorted_flairs.csv\")\n",
    "# create regular expression to filter known adoptees and non adoptees\n",
    "adoptee_labels = \"|\".join([re.escape(flair) for flair in sorted_flairs.adoptee_flair.dropna().values])\n",
    "non_adoptee_labels = \"|\".join([re.escape(flair) for flair in sorted_flairs.non_adoptee_flair.dropna().values])\n",
    "adoptee_pattern = re.compile(r\"{}\".format(adoptee_labels), re.IGNORECASE)\n",
    "non_adoptee_pattern = re.compile(r\"{}\".format(non_adoptee_labels), re.IGNORECASE)\n",
    "\n",
    "def categorize_user(x):\n",
    "    if adoptee_pattern.match(x):\n",
    "        return 1  # Adoptee\n",
    "    elif non_adoptee_pattern.match(x):\n",
    "        return 0  # Non-adoptee\n",
    "    else:\n",
    "        return -1  # NEI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [],
   "source": [
    "full_df.loc[:,\"is_adoptee\"] = full_df.author_flair.astype(str).apply(categorize_user).copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [],
   "source": [
    "id_to_author = full_df[[\"author\",\"id\"]].set_index(\"id\").author.to_dict()\n",
    "\n",
    "full_df[\"target\"] = full_df.parent_id.map(id_to_author)\n",
    "full_df[\"indirect_target\"] = full_df.link_id.map(id_to_author)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [],
   "source": [
    "full_df[['author', 'is_adoptee', 'full_text', \"subreddit\", \"target\",\"indirect_target\", \"num_comments\", \"score\"]].to_parquet('input.parquet', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>parent_id</th>\n",
       "      <th>id</th>\n",
       "      <th>link_id</th>\n",
       "      <th>author</th>\n",
       "      <th>author_flair</th>\n",
       "      <th>num_comments</th>\n",
       "      <th>time</th>\n",
       "      <th>title</th>\n",
       "      <th>text</th>\n",
       "      <th>score</th>\n",
       "      <th>subreddit</th>\n",
       "      <th>full_text</th>\n",
       "      <th>is_adoptee</th>\n",
       "      <th>target</th>\n",
       "      <th>indirect_target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>5k0hve</td>\n",
       "      <td>5k0hve</td>\n",
       "      <td>5k0hve</td>\n",
       "      <td>Shamaroo</td>\n",
       "      <td>NaN</td>\n",
       "      <td>13.0</td>\n",
       "      <td>2016-12-24 00:55:47</td>\n",
       "      <td>I have no clue if this is the right place but ...</td>\n",
       "      <td>My wife works at a children's hospital not goi...</td>\n",
       "      <td>6</td>\n",
       "      <td>Adoption</td>\n",
       "      <td>i have no clue if this is the right place but ...</td>\n",
       "      <td>-1</td>\n",
       "      <td>Shamaroo</td>\n",
       "      <td>Shamaroo</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>5k0hve</td>\n",
       "      <td>dbkdylx</td>\n",
       "      <td>5k0hve</td>\n",
       "      <td>surf_wax</td>\n",
       "      <td>Adoptee</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2016-12-24 01:17:51</td>\n",
       "      <td></td>\n",
       "      <td>Probably not going to happen.  If she was just...</td>\n",
       "      <td>7</td>\n",
       "      <td>Adoption</td>\n",
       "      <td>probably not going to happen. if she was just ...</td>\n",
       "      <td>1</td>\n",
       "      <td>Shamaroo</td>\n",
       "      <td>Shamaroo</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>dbkdylx</td>\n",
       "      <td>dbkxkh9</td>\n",
       "      <td>5k0hve</td>\n",
       "      <td>usernamebrainfreeze</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2016-12-24 14:37:07</td>\n",
       "      <td></td>\n",
       "      <td>Also if your willing to consider taking in a s...</td>\n",
       "      <td>3</td>\n",
       "      <td>Adoption</td>\n",
       "      <td>also if your willing to consider taking in a s...</td>\n",
       "      <td>-1</td>\n",
       "      <td>surf_wax</td>\n",
       "      <td>Shamaroo</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>dbkdylx</td>\n",
       "      <td>dbkragy</td>\n",
       "      <td>5k0hve</td>\n",
       "      <td>thismoment76</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2016-12-24 08:56:45</td>\n",
       "      <td></td>\n",
       "      <td>I know a family that went through fostering a ...</td>\n",
       "      <td>2</td>\n",
       "      <td>Adoption</td>\n",
       "      <td>i know a family that went through fostering a ...</td>\n",
       "      <td>-1</td>\n",
       "      <td>surf_wax</td>\n",
       "      <td>Shamaroo</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>dbkdylx</td>\n",
       "      <td>dbkg6si</td>\n",
       "      <td>5k0hve</td>\n",
       "      <td>Shamaroo</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2016-12-24 02:20:31</td>\n",
       "      <td></td>\n",
       "      <td>Hey thanks for the reply even tho it's sucks t...</td>\n",
       "      <td>2</td>\n",
       "      <td>Adoption</td>\n",
       "      <td>hey thanks for the reply even tho it is sucks ...</td>\n",
       "      <td>-1</td>\n",
       "      <td>surf_wax</td>\n",
       "      <td>Shamaroo</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>dbkg6si</td>\n",
       "      <td>dbkgybr</td>\n",
       "      <td>5k0hve</td>\n",
       "      <td>surf_wax</td>\n",
       "      <td>Adoptee</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2016-12-24 02:41:44</td>\n",
       "      <td></td>\n",
       "      <td>Yeah, I get it -- there are some awful people ...</td>\n",
       "      <td>4</td>\n",
       "      <td>Adoption</td>\n",
       "      <td>yeah i get it there are some awful people out ...</td>\n",
       "      <td>1</td>\n",
       "      <td>Shamaroo</td>\n",
       "      <td>Shamaroo</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>dbkdylx</td>\n",
       "      <td>dbqdrz2</td>\n",
       "      <td>5k0hve</td>\n",
       "      <td>Starrcraters</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2016-12-28 22:19:35</td>\n",
       "      <td></td>\n",
       "      <td>You'd be surprised how quickly reunification a...</td>\n",
       "      <td>1</td>\n",
       "      <td>Adoption</td>\n",
       "      <td>you would be surprised how quickly reunificati...</td>\n",
       "      <td>-1</td>\n",
       "      <td>surf_wax</td>\n",
       "      <td>Shamaroo</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>5k0hve</td>\n",
       "      <td>dbl7ipa</td>\n",
       "      <td>5k0hve</td>\n",
       "      <td>ThrowawayTink2</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2016-12-24 19:14:59</td>\n",
       "      <td></td>\n",
       "      <td>It actually sounds like this baby was removed ...</td>\n",
       "      <td>3</td>\n",
       "      <td>Adoption</td>\n",
       "      <td>it actually sounds like this baby was removed ...</td>\n",
       "      <td>-1</td>\n",
       "      <td>Shamaroo</td>\n",
       "      <td>Shamaroo</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>5k0hve</td>\n",
       "      <td>dbkpyc8</td>\n",
       "      <td>5k0hve</td>\n",
       "      <td>ralpher1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2016-12-24 07:46:16</td>\n",
       "      <td></td>\n",
       "      <td>Birth mom will need to consent to place the ch...</td>\n",
       "      <td>2</td>\n",
       "      <td>Adoption</td>\n",
       "      <td>bmom will need to consent to place the child f...</td>\n",
       "      <td>-1</td>\n",
       "      <td>Shamaroo</td>\n",
       "      <td>Shamaroo</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>dbkpyc8</td>\n",
       "      <td>dbqd9js</td>\n",
       "      <td>5k0hve</td>\n",
       "      <td>Starrcraters</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2016-12-28 22:08:02</td>\n",
       "      <td></td>\n",
       "      <td>Parents do not have to consent to adoption if ...</td>\n",
       "      <td>1</td>\n",
       "      <td>Adoption</td>\n",
       "      <td>parents do not have to consent to adoption if ...</td>\n",
       "      <td>-1</td>\n",
       "      <td>ralpher1</td>\n",
       "      <td>Shamaroo</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>5k0hve</td>\n",
       "      <td>dbkjxrg</td>\n",
       "      <td>5k0hve</td>\n",
       "      <td>Likemylife</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2016-12-24 04:07:53</td>\n",
       "      <td></td>\n",
       "      <td>If it is through the state usually the keep in...</td>\n",
       "      <td>1</td>\n",
       "      <td>Adoption</td>\n",
       "      <td>if it is through the state usually the keep in...</td>\n",
       "      <td>-1</td>\n",
       "      <td>Shamaroo</td>\n",
       "      <td>Shamaroo</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>5k0hve</td>\n",
       "      <td>dbktm5j</td>\n",
       "      <td>5k0hve</td>\n",
       "      <td>AdoptionQandA</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2016-12-24 11:17:55</td>\n",
       "      <td></td>\n",
       "      <td>She has family. They just can't afford the sur...</td>\n",
       "      <td>1</td>\n",
       "      <td>Adoption</td>\n",
       "      <td>she has family. they just can not afford the s...</td>\n",
       "      <td>-1</td>\n",
       "      <td>Shamaroo</td>\n",
       "      <td>Shamaroo</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>5k0hve</td>\n",
       "      <td>dbqcape</td>\n",
       "      <td>5k0hve</td>\n",
       "      <td>Starrcraters</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2016-12-28 21:46:45</td>\n",
       "      <td></td>\n",
       "      <td>I worked for DHS in another state (the laws va...</td>\n",
       "      <td>1</td>\n",
       "      <td>Adoption</td>\n",
       "      <td>i worked for dhs in another state the laws var...</td>\n",
       "      <td>-1</td>\n",
       "      <td>Shamaroo</td>\n",
       "      <td>Shamaroo</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>dbqcape</td>\n",
       "      <td>dbqct4b</td>\n",
       "      <td>5k0hve</td>\n",
       "      <td>Starrcraters</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2016-12-28 21:57:53</td>\n",
       "      <td></td>\n",
       "      <td>Remember DHS can't confirm or deny having an o...</td>\n",
       "      <td>1</td>\n",
       "      <td>Adoption</td>\n",
       "      <td>remember dhs can not confirm or deny having an...</td>\n",
       "      <td>-1</td>\n",
       "      <td>Starrcraters</td>\n",
       "      <td>Shamaroo</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>mfjhrk</td>\n",
       "      <td>mfjhrk</td>\n",
       "      <td>mfjhrk</td>\n",
       "      <td>xkiwified</td>\n",
       "      <td>NaN</td>\n",
       "      <td>11.0</td>\n",
       "      <td>2021-03-29 06:43:10</td>\n",
       "      <td>chinese adoptee</td>\n",
       "      <td>hi i was adopted from china when i was 13 mont...</td>\n",
       "      <td>7</td>\n",
       "      <td>Adoption</td>\n",
       "      <td>chinese adoptee. hi i was adopted from china w...</td>\n",
       "      <td>-1</td>\n",
       "      <td>xkiwified</td>\n",
       "      <td>xkiwified</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   parent_id       id link_id               author author_flair  num_comments  \\\n",
       "0     5k0hve   5k0hve  5k0hve             Shamaroo          NaN          13.0   \n",
       "1     5k0hve  dbkdylx  5k0hve             surf_wax      Adoptee           NaN   \n",
       "2    dbkdylx  dbkxkh9  5k0hve  usernamebrainfreeze          NaN           NaN   \n",
       "3    dbkdylx  dbkragy  5k0hve         thismoment76          NaN           NaN   \n",
       "4    dbkdylx  dbkg6si  5k0hve             Shamaroo          NaN           NaN   \n",
       "5    dbkg6si  dbkgybr  5k0hve             surf_wax      Adoptee           NaN   \n",
       "6    dbkdylx  dbqdrz2  5k0hve         Starrcraters          NaN           NaN   \n",
       "7     5k0hve  dbl7ipa  5k0hve       ThrowawayTink2          NaN           NaN   \n",
       "8     5k0hve  dbkpyc8  5k0hve             ralpher1          NaN           NaN   \n",
       "9    dbkpyc8  dbqd9js  5k0hve         Starrcraters          NaN           NaN   \n",
       "10    5k0hve  dbkjxrg  5k0hve           Likemylife          NaN           NaN   \n",
       "11    5k0hve  dbktm5j  5k0hve        AdoptionQandA          NaN           NaN   \n",
       "12    5k0hve  dbqcape  5k0hve         Starrcraters          NaN           NaN   \n",
       "13   dbqcape  dbqct4b  5k0hve         Starrcraters          NaN           NaN   \n",
       "14    mfjhrk   mfjhrk  mfjhrk            xkiwified          NaN          11.0   \n",
       "\n",
       "                  time                                              title  \\\n",
       "0  2016-12-24 00:55:47  I have no clue if this is the right place but ...   \n",
       "1  2016-12-24 01:17:51                                                      \n",
       "2  2016-12-24 14:37:07                                                      \n",
       "3  2016-12-24 08:56:45                                                      \n",
       "4  2016-12-24 02:20:31                                                      \n",
       "5  2016-12-24 02:41:44                                                      \n",
       "6  2016-12-28 22:19:35                                                      \n",
       "7  2016-12-24 19:14:59                                                      \n",
       "8  2016-12-24 07:46:16                                                      \n",
       "9  2016-12-28 22:08:02                                                      \n",
       "10 2016-12-24 04:07:53                                                      \n",
       "11 2016-12-24 11:17:55                                                      \n",
       "12 2016-12-28 21:46:45                                                      \n",
       "13 2016-12-28 21:57:53                                                      \n",
       "14 2021-03-29 06:43:10                                    chinese adoptee   \n",
       "\n",
       "                                                 text  score subreddit  \\\n",
       "0   My wife works at a children's hospital not goi...      6  Adoption   \n",
       "1   Probably not going to happen.  If she was just...      7  Adoption   \n",
       "2   Also if your willing to consider taking in a s...      3  Adoption   \n",
       "3   I know a family that went through fostering a ...      2  Adoption   \n",
       "4   Hey thanks for the reply even tho it's sucks t...      2  Adoption   \n",
       "5   Yeah, I get it -- there are some awful people ...      4  Adoption   \n",
       "6   You'd be surprised how quickly reunification a...      1  Adoption   \n",
       "7   It actually sounds like this baby was removed ...      3  Adoption   \n",
       "8   Birth mom will need to consent to place the ch...      2  Adoption   \n",
       "9   Parents do not have to consent to adoption if ...      1  Adoption   \n",
       "10  If it is through the state usually the keep in...      1  Adoption   \n",
       "11  She has family. They just can't afford the sur...      1  Adoption   \n",
       "12  I worked for DHS in another state (the laws va...      1  Adoption   \n",
       "13  Remember DHS can't confirm or deny having an o...      1  Adoption   \n",
       "14  hi i was adopted from china when i was 13 mont...      7  Adoption   \n",
       "\n",
       "                                            full_text  is_adoptee  \\\n",
       "0   i have no clue if this is the right place but ...          -1   \n",
       "1   probably not going to happen. if she was just ...           1   \n",
       "2   also if your willing to consider taking in a s...          -1   \n",
       "3   i know a family that went through fostering a ...          -1   \n",
       "4   hey thanks for the reply even tho it is sucks ...          -1   \n",
       "5   yeah i get it there are some awful people out ...           1   \n",
       "6   you would be surprised how quickly reunificati...          -1   \n",
       "7   it actually sounds like this baby was removed ...          -1   \n",
       "8   bmom will need to consent to place the child f...          -1   \n",
       "9   parents do not have to consent to adoption if ...          -1   \n",
       "10  if it is through the state usually the keep in...          -1   \n",
       "11  she has family. they just can not afford the s...          -1   \n",
       "12  i worked for dhs in another state the laws var...          -1   \n",
       "13  remember dhs can not confirm or deny having an...          -1   \n",
       "14  chinese adoptee. hi i was adopted from china w...          -1   \n",
       "\n",
       "          target indirect_target  \n",
       "0       Shamaroo        Shamaroo  \n",
       "1       Shamaroo        Shamaroo  \n",
       "2       surf_wax        Shamaroo  \n",
       "3       surf_wax        Shamaroo  \n",
       "4       surf_wax        Shamaroo  \n",
       "5       Shamaroo        Shamaroo  \n",
       "6       surf_wax        Shamaroo  \n",
       "7       Shamaroo        Shamaroo  \n",
       "8       Shamaroo        Shamaroo  \n",
       "9       ralpher1        Shamaroo  \n",
       "10      Shamaroo        Shamaroo  \n",
       "11      Shamaroo        Shamaroo  \n",
       "12      Shamaroo        Shamaroo  \n",
       "13  Starrcraters        Shamaroo  \n",
       "14     xkiwified       xkiwified  "
      ]
     },
     "execution_count": 115,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "full_df.head(15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 116,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "upload_to_s3('input.parquet', BUCKET_NAME, 'input.parquet')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Serial code that accomplishes the same thing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [],
   "source": [
    "stop_words = set(stopwords.words('english'))\n",
    "# add more stop words\n",
    "custom_stopwords = set([\"could've\", \"would've\", \"r\", \"u/\", \"u\", '/u' \"/r\" \"r/\", \"t\", 've', 's', 'm', \n",
    "                        'll', 'nt', 'd', 're', 'n', 'y', 'b', 'p', 'f', 'c', 'e', 'g', \n",
    "                        'h', 'j', 'k', 'l', 'o', 'q', 'v', 'w', 'x', 'z', 'a', 'i', \"gt\", \"amp\", \n",
    "                        \"like\", \"don\", \"just\", \"kinda\", \"want\", \"know\", \"think\", \"dosnt\",\n",
    "                        \"get\", \"say\", \"go\", \"make\", \"andor\", \"yo\", \"andme\", \"bc\", \"nah\",\n",
    "                        \"ect\",\"soo\", \"sooo\", \"soooo\", \"sooooo\", \"soooooo\", \"sooooooo\", \"eachother\",\n",
    "                        \"modmail\", \"cuz\", \"andnbsp\", \"los\", \"yoffe\", \"bc\", \"thier\", \"ou\", \"andnbsp\",\n",
    "                        \n",
    "                        \"ve\",\"alot\", \"atleast\", \"yall\", \"notall\", \"noone\", \"eithe\", \"hai\",\"tion\"])\n",
    "stop_words.update(custom_stopwords)\n",
    "stop_words.update(nlp.Defaults.stop_words)\n",
    "for stopword in stop_words:\n",
    "    nlp.vocab[stopword].is_stop = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_and_normalize(word_list, custom_stopwords=None):\n",
    "    \"\"\"\n",
    "    Tokenize and normalize text, with support for custom stopwords.\n",
    "\n",
    "    Args:\n",
    "    - word_list (list of str or str): List of words or a single string of text.\n",
    "    - custom_stopwords (set of str): Additional stopwords to consider.\n",
    "\n",
    "    Returns:\n",
    "    - tuple: tokenized (list of str), normalized (list of str)\n",
    "    \"\"\"\n",
    "\n",
    "    # Join the list into a single string if it's a list of words\n",
    "    if isinstance(word_list, list):\n",
    "        word_list = ' '.join(word_list)\n",
    "    \n",
    "    # Process the text with spaCy\n",
    "    doc = nlp(word_list.lower())\n",
    "    \n",
    "    # Define the default stopwords and update with custom ones if provided\n",
    "\n",
    " \n",
    "    # Tokenization and checking for punctuation\n",
    "    tokenized = [token.text for token in doc if not token.is_punct and token.text.strip()]\n",
    "    \n",
    "    # Normalization while removing stopwords, punctuation, and numbers\n",
    "    normalized = [str(token.lemma_) for token in doc \n",
    "                  if token.text not in custom_stopwords \n",
    "                  and not token.is_punct and not token.like_num and token.text.strip()]\n",
    "\n",
    "    return tokenized, normalized\n",
    "\n",
    "\n",
    "def tokenize_sents(word_list, model=nlp):\n",
    "    \"\"\"\n",
    "    Tokenize a list of words using a specified model.\n",
    "\n",
    "    Parameters:\n",
    "        word_list (list): A list of words to be tokenized into sentences.\n",
    "        model (Spacy model): the language model to be used for tokenization. \n",
    "            Defaults to nlp model.\n",
    "\n",
    "    Returns:\n",
    "        list: list of sentences extracted from the input text.\n",
    "    \"\"\"\n",
    "\n",
    "    doc = model(word_list)\n",
    "    sentences = [sent.text.strip() for sent in doc.sents]\n",
    "    return sentences\n",
    "\n",
    "\n",
    "# created by ChatGPT to help me fix an error with tokenized_sents\n",
    "def tokenize_edited(x, stop_words=None):\n",
    "\n",
    "    if x.strip():  # Check if x is non-empty and not just whitespace\n",
    "        tokenized = [tokenize_and_normalize(s, stop_words) for s in tokenize_sents(x)]\n",
    "        if tokenized:  # Ensure there is something to unzip\n",
    "            return zip(*tokenized)\n",
    "    # Return empty tuples if no content\n",
    "    return ([], [])\n",
    "\n",
    "\n",
    "def tag_sents_pos(sentences):\n",
    "    \"\"\"\n",
    "    function which replicates NLTK pos tagging on sentences.\n",
    "    \"\"\"\n",
    "    new_sents = []\n",
    "    for sentence in sentences:\n",
    "        new_sent = ' '.join(sentence)\n",
    "        new_sents.append(new_sent)\n",
    "    final_string = ' '.join(new_sents)\n",
    "    doc = nlp(final_string)\n",
    "\n",
    "    pos_sents = []\n",
    "    for sent in doc.sents:\n",
    "        pos_sent = []\n",
    "        for token in sent:\n",
    "            pos_sent.append((token.text, token.tag_))\n",
    "        pos_sents.append(pos_sent)\n",
    "\n",
    "    return pos_sents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/299864 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 299864/299864 [3:15:13<00:00, 25.60it/s]   \n",
      "100%|██████████| 299864/299864 [00:01<00:00, 160369.34it/s]\n",
      "100%|██████████| 299864/299864 [00:03<00:00, 90230.62it/s]\n",
      "100%|██████████| 299864/299864 [57:42<00:00, 86.61it/s]  \n"
     ]
    }
   ],
   "source": [
    "tqdm.pandas()\n",
    "full_df[\"token_sents\"], full_df[\"norm_sents\"] = zip(*full_df.full_text.progress_apply(lambda x: tokenize_edited(x, stop_words)))\n",
    "full_df[\"norm_tokens\"] = full_df.norm_sents.progress_apply(lambda x: [item for sublist in x for item in sublist])\n",
    "full_df[\"full_tokens\"] = full_df.token_sents.progress_apply(lambda x: [item for sublist in x for item in sublist])\n",
    "full_df['POS_sents'] = full_df.token_sents.progress_apply(lambda x: tag_sents_pos(x))\n",
    "full_df[\"num_tokens\"] = full_df.full_tokens.apply(len)\n",
    "full_df[\"num_norm_tokens\"] = full_df.norm_tokens.apply(len)\n",
    "\n",
    "# takes 253 minutes 7.8 seconds to run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [],
   "source": [
    "full_df.to_pickle(r\"serial_output.pkl\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "macs30123",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
