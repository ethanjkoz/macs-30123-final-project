{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%configure -f\n",
    "{\n",
    "    \"conf\": {\n",
    "        \"spark.jars.packages\": \"com.johnsnowlabs.nlp:spark-nlp_2.12:4.3.1\",\n",
    "        \"spark.pyspark.python\": \"python3\",\n",
    "        \"spark.pyspark.virtualenv.enabled\": \"true\",\n",
    "        \"spark.pyspark.virtualenv.type\":\"native\",\n",
    "        \"spark.pyspark.virtualenv.bin.path\":\"/usr/bin/virtualenv\"\n",
    "    }\n",
    "}\n",
    "\n",
    "sc.install_pypi_package('spark-nlp', \"https://pypi.org/simple\")\n",
    "sc.install_pypi_package('Cython==0.29.21', \"https://pypi.org/simple\")\n",
    "sc.install_pypi_package('pandas==1.1.5', \"https://pypi.org/simple\")\n",
    "# sc.install_pypi_package('scipy', \"https://pypi.org/simple\")\n",
    "# sc.install_pypi_package('scikit-learn', \"https://pypi.org/simple\")\n",
    "# sc.install_pypi_package('sentence-transformers', \"https://pypi.org/simple\")\n",
    "\n",
    "from pyspark.ml import Pipeline, PipelineModel, Transformer\n",
    "import pyspark.sql.functions as F\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.types import StructType, StructField, IntegerType, StringType, ArrayType, FloatType\n",
    "import sparknlp\n",
    "from sparknlp.pretrained import PretrainedPipeline\n",
    "from sparknlp.annotator import *\n",
    "from sparknlp.common import RegexRule\n",
    "from sparknlp.base import *\n",
    "import numpy as np\n",
    "import re\n",
    "from pyspark.ml.feature import VectorAssembler, StopWordsRemover, Word2Vec\n",
    "from pyspark.ml.linalg import Vectors, VectorUDT\n",
    "from pyspark.ml.util import Identifiable\n",
    "\n",
    "from pyspark.ml.classification import LogisticRegression, RandomForestClassifier\n",
    "from pyspark.ml.feature import VectorAssembler\n",
    "from pyspark.ml.tuning import ParamGridBuilder, CrossValidator\n",
    "from pyspark.ml.evaluation import BinaryClassificationEvaluator\n",
    "\n",
    "Let's start the pipeline\n",
    "\n",
    "Firstly, I want to create a list of common stop words that include oddities from the data itself (known from previous attempts at cleaning the data.\n",
    "\n",
    "# get stop words from PySpark NLP\n",
    "stop_words = set(StopWordsRemover().getStopWords())\n",
    "\n",
    "custom_stopwords = set([\"could've\", \"would've\", \"r\", \"u/\", \"u\", '/u' \"/r\" \"r/\", \"t\", 've', 's', 'm', \n",
    "                        'll', 'nt', 'd', 're', 'n', 'y', 'b', 'p', 'f', 'c', 'e', 'g', \"say\", \"go\", \n",
    "                        'h', 'j', 'k', 'l', 'o', 'q', 'v', 'w', 'x', 'z', 'a', 'i', \"gt\", \"amp\", \"us\",\n",
    "                        \"like\", \"don\", \"just\", \"kinda\", \"want\", \"know\", \"think\", \"dosnt\", \"couldnt\", \"wouldnt\",\n",
    "                        \"get\", \"andor\", \"andme\", \"doesnt\", \"ect\",\"soo\", \"sooo\", \"soooo\", \"sooooo\", \"though\", \"into\"\n",
    "                        \"unto\", \"onto\", \"meanwhile\",\"soooooo\", \"sooooooo\", \"eachother\", \"dont\", \"wont\", \"cant\", \n",
    "                        \"modmail\", \"cuz\", \"andnbsp\", \"los\", \"yoffe\", \"bc\", \"thier\", \"ou\", \"andnbsp\", \"shant\", \"shouldnt\",  \n",
    "                        \"ve\",\"alot\", \"atleast\", \"their\", \"thier\", \"yall\", \"notall\", \"noone\", \"eithe\", \"hai\",\"tion\"])\n",
    "# combine the stop words to one list\n",
    "stop_list = list(stop_words | custom_stopwords)\n",
    "\n",
    "# load in the data\n",
    "newer_df = spark.read.option(\"inferSchema\", \"true\") \\\n",
    "    .parquet('s3://ethan-kozlowski-project/input.parquet')\n",
    "newer_df.persist()\n",
    "\n",
    "Let's take a little look inside\n",
    "\n",
    "newer_df.printSchema()\n",
    "\n",
    "print(f\"Number of rows: {newer_df.count()}\")\n",
    "print(f\"Number of columns: {len(newer_df.columns)}\")\n",
    "\n",
    "We can see that there are around 300k entries. This includes posts and comments to these posts.  This data has been somewhat precleaned and has posts with missing authors removed. Additionally, the full_text column includes all text available for a post, including the title appended to the front when applicable. Additionally there is information on the subreddit and the subject (target) of the reply/comment. For original posts, the target, author, and indirect_target will all be the same person. \n",
    "\n",
    "newer_df.show(truncate=15)\n",
    "\n",
    "documentAssembler = DocumentAssembler()\\\n",
    "    .setInputCol(\"full_text\")\\\n",
    "    .setOutputCol(\"document\")\n",
    "\n",
    "# tokenize text\n",
    "tokenizer = Tokenizer() \\\n",
    "    .setInputCols([\"document\"]) \\\n",
    "    .setOutputCol(\"tokens\")\n",
    "\n",
    "# normalize tokens\n",
    "normalizer = Normalizer() \\\n",
    "    .setInputCols([\"tokens\"]) \\\n",
    "    .setOutputCol(\"norm_tokens\")\n",
    "\n",
    "# clean stopwords\n",
    "stopwords_cleaner = StopWordsCleaner() \\\n",
    "    .setInputCols([\"norm_tokens\"]) \\\n",
    "    .setOutputCol(\"clean_tokens\") \\\n",
    "    .setStopWords(stop_list)\n",
    "\n",
    "pos = PerceptronModel.pretrained(\"pos_anc\", 'en')\\\n",
    "        .setInputCols(\"document\", \"clean_tokens\")\\\n",
    "        .setOutputCol(\"pos\")\n",
    "\n",
    "\n",
    "# #  bert embeddings\n",
    "# bert_embeddings = BertEmbeddings.pretrained('bert_base_uncased', 'en') \\\n",
    "#     .setInputCols([\"document\", \"clean_tokens\"]) \\\n",
    "#     .setOutputCol(\"features\") \\\n",
    "#     .setCaseSensitive(False) \\\n",
    "#     .setMaxSentenceLength(512)\n",
    "\n",
    "# # required for classifier DL later\n",
    "# sentence_embeddings = SentenceEmbeddings()\\\n",
    "#     .setInputCols([\"document\", \"features\"])\\\n",
    "#     .setOutputCol(\"sentence_embeddings\")\\\n",
    "#     .setPoolingStrategy(\"AVERAGE\")\n",
    "\n",
    "# # deep learning classifier\n",
    "# classifier_dl = ClassifierDLApproach() \\\n",
    "#     .setInputCols([\"sentence_embeddings\"]) \\\n",
    "#     .setOutputCol(\"pred_cat_is_adoptee\") \\\n",
    "#     .setLabelColumn(\"is_adoptee\") \\\n",
    "#     .setMaxEpochs(5) \\\n",
    "#     .setEnableOutputLogs(True)\n",
    "\n",
    "\n",
    "\n",
    "class TokenCountTransformer(Transformer):\n",
    "    @keyword_only\n",
    "    def __init__(self, inputCol, outputCol):\n",
    "        super(TokenCountTransformer, self).__init__()\n",
    "        self.inputCol = inputCol\n",
    "        self.outputCol = outputCol\n",
    "\n",
    "    def _transform(self, ddf):\n",
    "        # get count of tokens\n",
    "        count_tokens = F.udf(lambda tokens: len(tokens), IntegerType())\n",
    "        return ddf.withColumn(self.outputCol, count_tokens(ddf[self.inputCol]))\n",
    "\n",
    "# count total tokens\n",
    "token_count_transformer = TokenCountTransformer(\n",
    "    inputCol=\"tokens\", \n",
    "    outputCol=\"num_tokens\")\n",
    "\n",
    "# count norm tokens    \n",
    "norm_token_count_transformer = TokenCountTransformer(\n",
    "    inputCol=\"clean_tokens\", \n",
    "    outputCol=\"num_norm_tokens\")\n",
    "\n",
    "\n",
    "finisher = Finisher() \\\n",
    "    .setInputCols([\"clean_tokens\"]) \\\n",
    "    .setOutputCols([\"finished_tokens\"]) \\\n",
    "    .setOutputAsArray(True) \\\n",
    "    .setCleanAnnotations(True)\n",
    "\n",
    "word2Vec = Word2Vec() \\\n",
    "    .setInputCol(\"finished_tokens\") \\\n",
    "    .setOutputCol(\"features\") \\\n",
    "    .setVectorSize(300) \\\n",
    "    .setMinCount(5) \\\n",
    "    .setWindowSize(5) \\\n",
    "    .setSeed(42)\n",
    "\n",
    "\n",
    "vector_pipeline = Pipeline(stages=[\n",
    "    documentAssembler,\n",
    "    tokenizer,\n",
    "    normalizer,\n",
    "    stopwords_cleaner,\n",
    "    pos,\n",
    "    token_count_transformer,\n",
    "    norm_token_count_transformer,\n",
    "#     bert_embeddings,\n",
    "#     sentence_embeddings,\n",
    "#     classifier_dl,\n",
    "    finisher,\n",
    "    word2Vec,\n",
    "])\n",
    "\n",
    "\n",
    "feature_df = vector_pipeline.fit(newer_df).transform(newer_df)\n",
    "\n",
    "feature_df[[\"is_adoptee\", \"features\"]].show(truncate=10)\n",
    "\n",
    "labeled_df = feature_df.filter(newer_df.is_adoptee != -1)\n",
    "train_df, test_df = labeled_df.randomSplit([0.8, 0.2], seed=42)\n",
    "train_df.persist()\n",
    "test_df.persist()\n",
    "\n",
    "# make the model pipeline too\n",
    "assembler = VectorAssembler(inputCols=[\"features\"], outputCol=\"assembled_features\")\n",
    "\n",
    "\n",
    "# let's try these models\n",
    "lr = LogisticRegression(featuresCol=\"assembled_features\", labelCol=\"is_adoptee\")\n",
    "rf = RandomForestClassifier(featuresCol=\"assembled_features\", labelCol=\"is_adoptee\")\n",
    "\n",
    "# Create pipelines\n",
    "lr_pipeline = Pipeline(stages=[assembler, lr])\n",
    "rf_pipeline = Pipeline(stages=[assembler, rf])\n",
    "\n",
    "\n",
    "# Define parameter grids\n",
    "lr_param_grid = (ParamGridBuilder()\n",
    "                 .addGrid(lr.regParam, [0.01, 0.1, 1.0])\n",
    "                 .addGrid(lr.elasticNetParam, [0.0, 0.5, 1.0])\n",
    "                 .build())\n",
    "\n",
    "rf_param_grid = (ParamGridBuilder()\n",
    "                 .addGrid(rf.numTrees, [10, 50, 100])\n",
    "                 .addGrid(rf.maxDepth, [5, 10, 20])\n",
    "                 .build())\n",
    "\n",
    "evaluator = BinaryClassificationEvaluator(labelCol=\"is_adoptee\", metricName=\"areaUnderROC\")\n",
    "\n",
    "# Set up cross validators\n",
    "lr_cv = CrossValidator(estimator=lr_pipeline,\n",
    "                       estimatorParamMaps=lr_param_grid,\n",
    "                       evaluator=evaluator,\n",
    "                       numFolds=5)\n",
    "\n",
    "rf_cv = CrossValidator(estimator=rf_pipeline,\n",
    "                       estimatorParamMaps=rf_param_grid,\n",
    "                       evaluator=evaluator,\n",
    "                       numFolds=5)\n",
    "\n",
    "# Train models with cross-validation\n",
    "lr_cv_model = lr_cv.fit(train_df)\n",
    "rf_cv_model = rf_cv.fit(train_df)\n",
    "\n",
    "# Evaluate logistic regression model\n",
    "lr_preds = lr_cv_model.transform(test_df)\n",
    "lr_auc = evaluator.evaluate(lr_preds)\n",
    "print(f\"Logistic Regression AUC: {lr_auc}\")\n",
    "\n",
    "# Evaluate random forest model\n",
    "rf_preds = rf_cv_model.transform(test_df)\n",
    "rf_auc = evaluator.evaluate(rf_preds)\n",
    "print(f\"Random Forest AUC: {rf_auc}\")\n",
    "\n",
    "# Save the best models\n",
    "lr_cv_model.bestModel.save(\"s3://ethan-kozlowski-project/lr_best_model\")\n",
    "rf_cv_model.bestModel.save(\"s3://ethan-kozlowski-project/rf_best_model\")\n",
    "\n",
    "# Save DataFrame to S3 in Parquet format\n",
    "result.write.mode(\"overwrite\").parquet(\"s3://ethan-kozlowski-project/output\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
